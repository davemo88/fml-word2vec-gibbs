\documentclass[]{article}

%opening
\title{Bregman Divergence and word2vec}
\author{Jonay Trenous, David Kasofsky}

\begin{document}

\maketitle

\begin{abstract}

\noindent The word-embedding models of Mikolov et al.\cite{word2vec1}\cite{word2vec2} are related to Bregman Divergence by the softmax function. The parametrized distribution resulting from the softmax is a Gibbs distribution, which arises from the choice of KL-divergence as a similarity measure. For maximum entropy models, is known that the distribution that minimizes the KL-divergence is same Gibbs distribution that maximizes the likelihood of the data.

Here we can go a few ways with this:


\begin{itemize}
	\item show similar equivalence i.e. show word2vec to minimize the KL-divergence between two distributions. perhaps this will improve the characterization of the word embeddings (or show that in this case it is not equivalent)
	\item use the exponential distribution corresponding with another bregman divergance to perform the normalization in word2vec
\end{itemize}

\end{abstract}

\section{Introduction}



\begin{thebibliography}{1}
\bibitem{word2vec1} Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. {\em Efficient Estimation of Word Representations in Vector Space} arXiv:1301.3781v3 [cs.CL] 7 Sep 2013

\bibitem{word2vec2} Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean. {\em Distributed Representations of Words and Phrases and their Compositionality} arXiv:1310.4546v1  [cs.CL]  16 Oct 2013

\bibitem{word2vec_explained} Yoav Goldberg, Omar Levy. {\em word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method} arXiv:1402.3722v1  [cs.CL]  15 Feb 2014

\end{thebibliography}

\end{document}