\documentclass[]{article}

%opening
\title{Bregman Divergence and word2vec}
\author{Jonay Trenous, David Kasofsky}

\usepackage{amsmath}

\usepackage{amssymb}

\usepackage{fancyhdr}

\usepackage{tikz}

\usepackage{pgfplots}

\usepackage{float}

\usepackage[]{algorithm2e} 
\usepackage{hyperref}
\usepackage[style=numeric]{biblatex}
\addbibresource{bib.bib}
\pagestyle{fancy}
\setlength{\parindent}{0in}
\begin{document}

\maketitle

\begin{abstract}

\noindent The word-embedding models of Mikolov et al.\cite{word2vec1}\cite{word2vec2} are related to Bregman Divergence by the softmax function. The parametrized distribution resulting from the softmax is a Gibbs distribution, which arises from the choice of KL-divergence as a similarity measure. For maximum entropy models, it is known that the distribution that minimizes the KL-divergence is same Gibbs distribution that maximizes the likelihood of the data. From here we a give maximum entropy interpretation of word2vec using conditional relative entropy and an equivalent objective function.

\end{abstract}

\section{Introduction}

word2vec\cite{word2vec1}\cite{word2vec2} is a recent promising word-embedding algorithm. It seeks to find vector representation of words in a corpus such that similar words (in the sense of meaning and usage) will have similar embeddings. This quality is merely a loose notion about what makes for good word embeddings and not a technical condition. Nonetheless, word2vec has produced some encouraging results\cite{dummy} that hint at the supposed quality of its word embeddings. A common example is that the embedding of, say, "France" would not be far from the embedding of "Spain"\cite{dummy} and that relations such as:\\

$w2v(France) + w2v(Paris) \simeq w2v(Spain) + w2v(Madrid)$\\

\noindent may hold, where $w2v(\cdot)$ is the embedding.

word2vec uses a skipgram model that predicts the context of a word given the word itself. A context of size $R$ comprises the $R$ words before and after the word in question and so each word in the corpus generates $2R$ word-context pairs to be classified. This is presented as the following optimization objective:\\

\begin{equation} \underset{W,C}{\text{argmax}} \underset{i \in m}{\Pi}{p(y_i|x_i; W, \Phi)}\label{eq:gibbs}\end{equation} 

\noindent where $p$ is the softmax function parameterized by $W$ and $\Phi$:\\

$p(y_i|x_i; W,\Phi) = \frac{e^{W(x_i) \cdot \Phi(y_i)}}{\sum_{(x,y) \in S}e^{W(x) \cdot \Phi(y)}}$.\\

$W$ is the word-embedding function which maps a word $x_i$ to a $D$-dimensional vector, where $D$ is the dimension we have chosen for embedding. Similarly $\Phi$ is the context embedding function, which maps a context $y_i$ to a $D$-dimensional vector. One can also imagine $W$ and $\Phi$ as matrices with size $V \times D$, where $V$ is the size of the vocabulary. Then $W(x_i)$ corresponds to the column of the matrix containing the embedding for $x_i$.

Thus word2vec seeks a Maximum Likelihood estimator of the sample $S$. However this objective is not convex as neither $W$ nor $\Phi$ are fixed. When either is fixed this becomes the well-known objective from logistic regression, which is convex and equivalent to Maximum Entropy\cite{dummy}. We will show that there is a corresponding Maximum Entropy interpretation in the non-convex case as well.

\section{Maximum Entropy}

In the Maximum Entropy formulation, we pose the objective:\\

\begin{equation}\underset{p(\cdot | x) \in \Delta, \Phi \in \mathbf{F}}{\text{argmin}} \quad E \left[D(p(\cdot|x) \vert \vert  p_0(\cdot|x))\right]\label{eq:maxent}
\end{equation}
\\

\noindent subject to\\

\begin{align}\underset{x \sim \hat{p}}{E}\left[\underset{y \sim p(\cdot|x)}{E}\left[\Phi(x,y)\right]\right] = \underset{(x,y) \in S}{E}\left[\Phi(x,y)\right]
\end{align}

\noindent where $p_0$ is the uniform distribution and $\hat{p}$ is the sample distribution. Note that, as opposed to the classical maximum entropy objective, the feature function $\Phi$ itself is now a parameter to be optimized. It is well known that the classical maximum entropy objective can be reformulated as a Gibbs-parametrized maximum likelihood problem. A very broad outline of the proof goes as follows
\begin{itemize}
\item First, incorporate the constraints in the objective with the use of Lagrangian multipliers
\item Set the gradients of the resulting unconstrained objective to zero
\item Find that the solution must be a Gibbs distribution that maximizes the data likelihood  
\end{itemize}
For a concise yet detailed derivation of this result, consult [XX]. \\

When fixing either $\Phi$ or $p(\cdot|x)$ in \autoref{eq:maxent}, the objectiv ebecomes equivalent to the classical maximum entropy objective. Therefore we can conlude that the partial derivatives of $G$ and $H$ w.r.t $W$ and $\Phi$ will vanish for the same $W, \Phi$:
\begin{align*}
\dfrac{\partial G}{\partial W} &= 0
 &\text{iff}& 
&\dfrac{\partial H}{\partial W} &= 0  
\intertext{and:}
\dfrac{\partial G}{\partial \Phi} &= 0 
 &\text{iff}& 
&\dfrac{\partial H}{\partial \Phi} &= 0  
\end{align*}  
These properties tell us that the gradient of $H$
\[\triangledown H = \left(\dfrac{\partial H}{\partial W},\dfrac{\partial H}{\partial \Phi}\right)\]
will vanish if and only if the gradient of $G$ vanishes. Therefore, we can conclude that the two objectives have the same extrema as a vanishing gradient is a necessary condition for local extrema.
\noindent The 

\printbibliography
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
