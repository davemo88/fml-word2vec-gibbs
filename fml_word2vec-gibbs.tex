\documentclass[]{article}

%opening
\title{Bregman Divergence and word2vec}
\author{Jonay Trenous, David Kasofsky}

\usepackage{amsmath}

\usepackage{amssymb}

\usepackage{fancyhdr}

\usepackage{tikz}

\usepackage{pgfplots}

\usepackage{float}

\usepackage[]{algorithm2e} 

\pagestyle{fancy}

\begin{document}

\maketitle

\begin{abstract}

\noindent The word-embedding models of Mikolov et al.\cite{word2vec1}\cite{word2vec2} are related to Bregman Divergence by the softmax function. The parametrized distribution resulting from the softmax is a Gibbs distribution, which arises from the choice of KL-divergence as a similarity measure. For maximum entropy models, it is known that the distribution that minimizes the KL-divergence is same Gibbs distribution that maximizes the likelihood of the data. From here we a give maximum entropy interpretation of word2vec using conditional relative entropy and an equivalent objective function.

\end{abstract}

\section{Introduction}

word2vec\cite{word2vec1}\cite{word2vec2} is a recent promising word-embedding algorithm. It seeks to find vector representation of words in a corpus such that similar words (in the sense of meaning and usage) will have similar embeddings. This quality is merely a loose notion about what makes for good word embeddings and not a technical condition. Nonetheless, word2vec has produced some encouraging results\cite{dummy} that hint at the supposed quality of its word embeddings. A common example is that the embedding of, say, "France" would not be far from the embedding of "Spain"\cite{dummy} and that relations such as:\\

$w2v(France) + w2v(Paris) \simeq w2v(Spain) + w2v(Madrid)$\\

\noindent may hold, where $w2v(\cdot)$ is the embedding.

word2vec uses a skipgram model that predicts the context of a word given the word itself. A context of size $R$ comprises the $R$ words before and after the word in question and so each word in the corpus generates $2R$ word-context pairs to be classified. This is presented as the following optimization objective:\\

$\underset{W,C}{\text{argmax}} \underset{i \in m}{\Pi}{p(y_i|x_i; W, \Phi)}$\\

\noindent where $x_i$ is a word, $y_i$ a context of $x_i$, and $p$ is the softmax function parameterized by $W$ and $\Phi$:\\

$p(y_i|x_i; W,\Phi) = \frac{e^{W(x_i) \cdot \Phi(y_i)}}{\sum_{(x,y) \in S}e^{W(x) \cdot \Phi(y)}}$.\\

$W$ is the word-embedding function which maps a word $x_i$ to a $D$-dimensional vector, where $D$ is the dimension we have chosen for embedding. Similarly $\Phi$ is the context embedding function, which maps a context $y_i$ to a $D$-dimensional vector. One can also imagine $W$ and $\Phi$ as matrices with size $V \times D$, where $V$ is the size of the vocabulary of our sample $S$. Then $W(x_i)$ corresponds to the column of the matrix W containing the embedding for $x_i$.

Thus word2vec is solving a maximum likelihood problem over Gibbs distributions, as the softmax defines a Gibbs distribution. However this objective is not convex as both $W$ and $\Phi$ are unfixed. When either is fixed this becomes the well-known objective from logistic regression, which is convex and equivalent to Maximum Entropy\cite{dummy}. We will show that there is a corresponding Maximum Entropy interpretation in the non-convex case as well. 

\section{Maximum Entropy}

In the Maximum Entropy formulation, we pose the objective:\\

$\underset{p(\cdot | x) \in \Delta, \Phi \in F}{\text{argmin}} \quad E \left[D(p(\cdot|x) \vert \vert  p_0(\cdot|x))\right]$\\

\noindent with\\

$\underset{x \sim \hat{p}}{E}\left[\underset{y \sim p(\cdot|x)}{E}\left[\Phi(x,y)\right]\right] = \underset{(x,y) \in S}{E}\left[\Phi(x,y)\right]$.\\

\noindent where $p_0$ is the uniform distribution and $\hat{p}$ is the sample distribution. Another way to express that condition is\\

$\forall (x,y) \in S: \hat{p}(y|x) = p(y|x; W,C)$\\

\noindent Since the objective are not convex, we will 

\begin{thebibliography}{1}
\bibitem{word2vec1} Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. {\em Efficient Estimation of Word Representations in Vector Space} arXiv:1301.3781v3 [cs.CL] 7 Sep 2013

\bibitem{word2vec2} Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean. {\em Distributed Representations of Words and Phrases and their Compositionality} arXiv:1310.4546v1  [cs.CL]  16 Oct 2013

\bibitem{word2vec_explained} Yoav Goldberg, Omar Levy. {\em word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method} arXiv:1402.3722v1  [cs.CL]  15 Feb 2014

\end{thebibliography}

\end{document}